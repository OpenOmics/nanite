# Python standard library
from os.path import join
import os, sys

# Local imports
from scripts.common import (
    allocated,
    provided, 
    references,
    str_bool
)

# Global workflow variables
configfile: 'config.json'                      # Generated from user input and config/*.json
workpath = config['project']['workpath']       # Pipeline's output directory
tmpdir   = config['options']['tmp_dir']        # Temporary directory
samples2barcodes = config['barcodes']          # Samples to demultiplex, `cat` together
# Find list of sample which 
# have mulitple barcodes, this 
# means they need to be merged  
barcoded_samples = [k for k in samples2barcodes if samples2barcodes[k]]
samples = list(config['barcodes'].keys())
# Nanofilt read average quality score filter
quality_filter = int(
    config['options']['quality_filter']
) # Default: 8

# Final output files of the pipeline,
# Rule DAG built from listed here 
rule all:
    input:
        # Merge samples with multiple barcodes,
        # @imported from `rule setup` in rules/trim.smk 
        expand(
            join(workpath, "{name}", "fastqs", "{name}.fastq.gz"), 
            name=samples
        ),
        # Base-calling quality filtering,
        # @imported from `rule nanofilt` in rules/trim.smk 
        expand(
            join(workpath, "{name}", "fastqs", "{name}.filtered.fastq.gz"),
            name=samples
        ),
        # Align against entire NCBI viral database,
        # @import from `rule minimap2` in rules/map.smk
        expand(
            join(workpath, "{name}", "bams", "{name}.sam"),
            name=samples
        ),
        # Create Krona report,
        # @import from `rule kronatools` in rules/report.smk
        expand(
            join(workpath, "{name}", "reports", "{name}_classification.html"),
            name=samples
        )


# Import rules 
include: join("rules", "common.smk")
include: join("rules", "trim.smk")
include: join("rules", "map.smk")
include: join("rules", "report.smk")